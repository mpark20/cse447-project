{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing input datasets\n",
    "* https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n",
    "* https://nlp.cs.washington.edu/xorqa/ \n",
    "\n",
    "Sections 1-2 process the two datasets into common json format so that we can retain\n",
    "metadata about the distribution of languages and source datasets in our combined intermediate dataset.\n",
    "\n",
    "The last section processes the combined json dataset into prefixes + next-chars and \n",
    "saves them as txt files, with each example on a single line. THIS IS WHAT WE WILL USE\n",
    "FOR TRAINING + PREDICTIONS IN `src/myprogram` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cornell Movie Dialogs Corpus\n",
    "(English-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/Cornell-University/movie-dialog-corpus?dataset_version_number=1&file_name=movie_titles_metadata.tsv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41.0k/41.0k [00:00<00:00, 563kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/Cornell-University/movie-dialog-corpus?dataset_version_number=1&file_name=movie_lines.tsv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7.95M/7.95M [00:00<00:00, 14.8MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting zip of movie_lines.tsv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/molly/.cache/kagglehub/datasets/Cornell-University/movie-dialog-corpus/versions/1/movie_titles_metadata.tsv\n",
      "/Users/molly/.cache/kagglehub/datasets/Cornell-University/movie-dialog-corpus/versions/1/movie_lines.tsv\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "metadata_path = kagglehub.dataset_download(\"Cornell-University/movie-dialog-corpus\", path=\"movie_titles_metadata.tsv\")\n",
    "movie_lines_path = kagglehub.dataset_download(\"Cornell-University/movie-dialog-corpus\", path=\"movie_lines.tsv\")\n",
    "print(metadata_path)\n",
    "print(movie_lines_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/molly/.cache/kagglehub/datasets/Cornell-University/movie-dialog-corpus/versions/1/movie_lines.tsv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# moved the downloaded files to project directory bc it saves to local cache by default\n",
    "# cp /Users/molly/.cache/kagglehub/datasets/Cornell-University/movie-dialog-corpus/versions/1/movie_lines.tsv ../raw_data/movie_lines.tsv\n",
    "# cp /Users/molly/.cache/kagglehub/datasets/Cornell-University/movie-dialog-corpus/versions/1/movie_titles_metadata.tsv ../raw_data/movie_titles_metadata.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>movie_title</th>\n",
       "      <th>movie_year</th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>imdb_votes</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m3</td>\n",
       "      <td>2001: a space odyssey</td>\n",
       "      <td>1968</td>\n",
       "      <td>8.4</td>\n",
       "      <td>163227.0</td>\n",
       "      <td>['adventure' 'mystery' 'sci-fi']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>m5</td>\n",
       "      <td>the fifth element</td>\n",
       "      <td>1997</td>\n",
       "      <td>7.5</td>\n",
       "      <td>133756.0</td>\n",
       "      <td>['action' 'adventure' 'romance' 'sci-fi' 'thri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>m9</td>\n",
       "      <td>the atomic submarine</td>\n",
       "      <td>1959</td>\n",
       "      <td>4.9</td>\n",
       "      <td>513.0</td>\n",
       "      <td>['sci-fi' 'thriller']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>m12</td>\n",
       "      <td>airplane ii: the sequel</td>\n",
       "      <td>1982</td>\n",
       "      <td>5.8</td>\n",
       "      <td>15210.0</td>\n",
       "      <td>['comedy' 'romance' 'sci-fi']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>m14</td>\n",
       "      <td>alien nation</td>\n",
       "      <td>1988</td>\n",
       "      <td>6.1</td>\n",
       "      <td>5590.0</td>\n",
       "      <td>['crime' 'drama' 'sci-fi' 'thriller']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id              movie_title movie_year  imdb_rating  imdb_votes  \\\n",
       "3        m3    2001: a space odyssey       1968          8.4    163227.0   \n",
       "5        m5        the fifth element       1997          7.5    133756.0   \n",
       "9        m9     the atomic submarine       1959          4.9       513.0   \n",
       "12      m12  airplane ii: the sequel       1982          5.8     15210.0   \n",
       "14      m14             alien nation       1988          6.1      5590.0   \n",
       "\n",
       "                                               genres  \n",
       "3                    ['adventure' 'mystery' 'sci-fi']  \n",
       "5   ['action' 'adventure' 'romance' 'sci-fi' 'thri...  \n",
       "9                               ['sci-fi' 'thriller']  \n",
       "12                      ['comedy' 'romance' 'sci-fi']  \n",
       "14              ['crime' 'drama' 'sci-fi' 'thriller']  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_metadata = pd.read_csv(\"../raw_data/movie_titles_metadata.tsv\", sep='\\t', header=None)\n",
    "movie_metadata.columns = [\"movie_id\", \"movie_title\", \"movie_year\", \"imdb_rating\", \"imdb_votes\", \"genres\"]\n",
    "\n",
    "# filter to sci-fi movies\n",
    "scifi_filter = movie_metadata['genres'].apply(lambda x: ('sci-fi' in x) if isinstance(x, str) else False)\n",
    "scifi_df = movie_metadata[scifi_filter]\n",
    "print(len(scifi_df))\n",
    "scifi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38025 lines, 111 movies\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>line_id</th>\n",
       "      <th>character_id</th>\n",
       "      <th>character_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m3</td>\n",
       "      <td>L3778</td>\n",
       "      <td>u55</td>\n",
       "      <td>FLOYD</td>\n",
       "      <td>We're trying to get there. I hope we can.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m3</td>\n",
       "      <td>L3771</td>\n",
       "      <td>u55</td>\n",
       "      <td>FLOYD</td>\n",
       "      <td>I'm sorry Dr. Smyslov but I'm really not at li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m3</td>\n",
       "      <td>L3757</td>\n",
       "      <td>u55</td>\n",
       "      <td>FLOYD</td>\n",
       "      <td>How did they manage to do that without any com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>m3</td>\n",
       "      <td>L3750</td>\n",
       "      <td>u55</td>\n",
       "      <td>FLOYD</td>\n",
       "      <td>Well I suppose they've been having a bit of tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>m3</td>\n",
       "      <td>L3729</td>\n",
       "      <td>u55</td>\n",
       "      <td>FLOYD</td>\n",
       "      <td>She's wonderful.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  movie_id line_id character_id character_name  \\\n",
       "0       m3   L3778          u55          FLOYD   \n",
       "2       m3   L3771          u55          FLOYD   \n",
       "4       m3   L3757          u55          FLOYD   \n",
       "6       m3   L3750          u55          FLOYD   \n",
       "8       m3   L3729          u55          FLOYD   \n",
       "\n",
       "                                                text  \n",
       "0          We're trying to get there. I hope we can.  \n",
       "2  I'm sorry Dr. Smyslov but I'm really not at li...  \n",
       "4  How did they manage to do that without any com...  \n",
       "6  Well I suppose they've been having a bit of tr...  \n",
       "8                                   She's wonderful.  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get movie lines from the above sci-fi movies that are longer than 10 chars\n",
    "movie_lines = pd.read_csv(\"../raw_data/movie_lines.tsv\", sep='\\t', on_bad_lines='skip', header=None)\n",
    "movie_lines.columns = [\"line_id\", \"character_id\", \"movie_id\", \"character_name\", \"text\"]\n",
    "scifi_movie_lines = pd.merge(scifi_df['movie_id'], movie_lines, on='movie_id', how='inner')\n",
    "length_filter = scifi_movie_lines['text'].apply(lambda x: isinstance(x, str) and len(x) > 10)\n",
    "\n",
    "scifi_movie_lines = scifi_movie_lines[length_filter]\n",
    "print(f\"{len(scifi_movie_lines)} lines, {len(scifi_movie_lines['movie_id'].unique())} movies\")\n",
    "scifi_movie_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"We're trying to get there. I hope we can.\",\n",
       "  'lang': 'en',\n",
       "  'source': 'cornell_movie_dialogs'},\n",
       " {'text': \"I'm sorry Dr. Smyslov but I'm really not at liberty to discuss this.\",\n",
       "  'lang': 'en',\n",
       "  'source': 'cornell_movie_dialogs'},\n",
       " {'text': 'How did they manage to do that without any communication?',\n",
       "  'lang': 'en',\n",
       "  'source': 'cornell_movie_dialogs'},\n",
       " {'text': \"Well I suppose they've been having a bit of trouble with some of the equipment.\",\n",
       "  'lang': 'en',\n",
       "  'source': 'cornell_movie_dialogs'},\n",
       " {'text': \"She's wonderful.\", 'lang': 'en', 'source': 'cornell_movie_dialogs'},\n",
       " {'text': \"I'm afraid I've only got a few minutes but I'd love to.\",\n",
       "  'lang': 'en',\n",
       "  'source': 'cornell_movie_dialogs'},\n",
       " {'text': \"Well how nice to see you again Elena. You're looking wonderful.\",\n",
       "  'lang': 'en',\n",
       "  'source': 'cornell_movie_dialogs'},\n",
       " {'text': 'SPACE STATTION 5 - LOUNGE',\n",
       "  'lang': 'en',\n",
       "  'source': 'cornell_movie_dialogs'},\n",
       " {'text': \"I think I'll have to go out and burn them off.\",\n",
       "  'lang': 'en',\n",
       "  'source': 'cornell_movie_dialogs'},\n",
       " {'text': 'I guess not.', 'lang': 'en', 'source': 'cornell_movie_dialogs'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currently in use: dump as json\n",
    "json_data = []\n",
    "for i,row in scifi_movie_lines.iterrows():\n",
    "  json_data.append({\n",
    "    \"text\": row[\"text\"],\n",
    "    \"lang\": \"en\",\n",
    "    \"source\": \"cornell_movie_dialogs\"\n",
    "  })\n",
    "\n",
    "json_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data_path = \"../data/scifi_movie_lines.jsonl\"\n",
    "overwrite = False\n",
    "if not os.path.exists(out_data_path) or overwrite:\n",
    "  with open(out_data_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in json_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "else:\n",
    "  print(\"data already loaded, no files changed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. XOR QA \n",
    "(Arabic, Bengali, Finnish, Japanese, Korean, Russian, and Telugu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'উইকিলিকস কত সালে সর্বপ্রথম ইন্টারনেটে প্রথম তথ্য প্রদর্শন করে ?',\n",
       "  'lang': 'bn',\n",
       "  'source': 'xorqa'},\n",
       " {'text': 'দ্বিতীয় বিশ্বযুদ্ধে কোন দেশ পরাজিত হয় ?',\n",
       "  'lang': 'bn',\n",
       "  'source': 'xorqa'},\n",
       " {'text': 'মার্কিন যুক্তরাষ্ট্রের সংবিধান অনুযায়ী মার্কিন যুক্তরাষ্ট্রে পুরুষ সমকামী বিবাহ কি আইনত বৈধ ?',\n",
       "  'lang': 'bn',\n",
       "  'source': 'xorqa'},\n",
       " {'text': 'আরব-ইসরায়েলি যুদ্ধে আরবের মোট কয়জন সৈন্যের মৃত্যু হয়েছিল ?',\n",
       "  'lang': 'bn',\n",
       "  'source': 'xorqa'},\n",
       " {'text': 'বিশ্বে প্রথম পুঁজিবাদী সমাজ কবে গড়ে ওঠে ?',\n",
       "  'lang': 'bn',\n",
       "  'source': 'xorqa'},\n",
       " {'text': 'প্রথম বিশ্বযুদ্ধের আনুষ্ঠানিক সূচনা কবে হয় ?',\n",
       "  'lang': 'bn',\n",
       "  'source': 'xorqa'},\n",
       " {'text': 'মানুষের উদ্ভবের ও বিকাশের বিবর্তন তত্ত্বটির প্রথম বিরোধিতা কে করেন ?',\n",
       "  'lang': 'bn',\n",
       "  'source': 'xorqa'},\n",
       " {'text': 'মানুষের উদ্ভবের ও বিকাশের বিবর্তন তত্ত্বটির উদ্ভাবক কে ?',\n",
       "  'lang': 'bn',\n",
       "  'source': 'xorqa'},\n",
       " {'text': 'মার্কিন যুক্তরাষ্ট্রের রিপাবলিকান পার্টির প্রথম প্রেসিডেন্ট কে ছিলেন ?',\n",
       "  'lang': 'bn',\n",
       "  'source': 'xorqa'},\n",
       " {'text': 'আরব–ইসরায়েল যুদ্ধের সময় আরবের মোট কত জন সৈন্যের মৃত্যু হয় ?',\n",
       "  'lang': 'bn',\n",
       "  'source': 'xorqa'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_data_path = \"../raw_data/xor_train_full.jsonl\"\n",
    "\n",
    "json_data = []\n",
    "with open(in_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "   for line in f:\n",
    "      jline=json.loads(line)\n",
    "      json_data.append({\n",
    "         \"text\": jline[\"question\"],\n",
    "         \"lang\": jline[\"lang\"],\n",
    "         \"source\": \"xorqa\"\n",
    "      })\n",
    "print(len(json_data))\n",
    "json_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data_path = \"../data/xor_qa.jsonl\"\n",
    "sample_json_data = random.sample(json_data, k=10000)\n",
    "overwrite = True\n",
    "if not os.path.exists(out_data_path) or overwrite:\n",
    "  with open(out_data_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in sample_json_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "else:\n",
    "  print(\"data already loaded, no files changed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. DailyDialog\n",
    "(English, Italian, German, and Chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_to_json(input_file, language, source):\n",
    "    with open(input_file, 'r') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    sentences = re.split(r' __eou__ |[\\n.]+', content)\n",
    "\n",
    "    json_data = []\n",
    "\n",
    "    for line in sentences:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            json_data.append({\n",
    "            'text': line,\n",
    "            'lang': language,\n",
    "            'source': source\n",
    "            })\n",
    "\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data_path = '../json_data/dailydialog.json'\n",
    "input_files = ['dialogues_text_En.txt', 'dialogues_text_It.txt', 'dialogues_text_Zh.txt', 'dialogues_text_De.txt']\n",
    "languages = ['en', 'it', 'zh', 'de']\n",
    "source = 'dailydialog'\n",
    "\n",
    "combined_json = []\n",
    "\n",
    "for input_file, language in zip(input_files, languages):\n",
    "  if os.path.exists(input_file):\n",
    "    language_json = process_text_to_json(input_file, language, source)\n",
    "    combined_json.extend(language_json)\n",
    "  else:\n",
    "    print('input file ', input_file, 'does not exist')\n",
    "\n",
    "# with open('dailydialog.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(combined_json, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "if not os.path.exists(out_data_path) or overwrite:\n",
    "  with open(out_data_path, 'w', encoding='utf-8') as f:\n",
    "    for item in combined_json:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "else:\n",
    "  print(\"data already loaded, no files changed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine everything into a single json dataset with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_json_data = []\n",
    "with open(\"../json_data/scifi_movie_lines.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "   for line in f:\n",
    "      jline=json.loads(line)\n",
    "      all_json_data.append(jline)\n",
    "with open(\"../json_data/xor_qa.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "   for line in f:\n",
    "      jline=json.loads(line)\n",
    "      all_json_data.append(jline)\n",
    "with open(\"../json_data/dailydialog.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "   for line in f:\n",
    "      jline=json.loads(line)\n",
    "      all_json_data.append(jline)\n",
    "\n",
    "random.shuffle(all_json_data)\n",
    "\n",
    "val_start = int(0.8 * len(all_json_data))\n",
    "test_start = int(0.9 * len(all_json_data))\n",
    "train_data = all_json_data[:val_start]\n",
    "val_data = all_json_data[val_start:test_start]\n",
    "test_data = all_json_data[test_start:]\n",
    "\n",
    "for split,data in zip([\"train\", \"val\", \"test\"], [train_data, val_data, test_data]):\n",
    "   with open(f\"../json_data/{split}_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "      for item_to_write in data:\n",
    "         f.write(json.dumps(item_to_write) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to prefix-suffix form for final dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    punct_removal = str.maketrans({c:\"\" for c in set(string.punctuation) if c != \".\"})\n",
    "    text = text.translate(punct_removal)\n",
    "    text = text.replace('\\n', ' ').replace('\\r','').replace('\\t', '')\n",
    "    text = text.strip()\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def make_dataset(json_data_path: str):\n",
    "  all_lines = []\n",
    "  with open(json_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "      for line in f:\n",
    "          item = json.loads(line)\n",
    "          all_lines.append(item[\"text\"])\n",
    "\n",
    "  # break lines into prefixes + next-char\n",
    "  all_prefixes = []\n",
    "  all_next_chars = []\n",
    "  min_len = 3\n",
    "  for line in all_lines:\n",
    "      prefixes = [line[:i+1] + \"\\n\" for i in range(min_len, len(line)-1)]\n",
    "      next_chars = [line[i] + \"\\n\" for i in range(min_len + 1, len(line))]\n",
    "      all_prefixes.extend(prefixes)\n",
    "      all_next_chars.extend(next_chars)\n",
    "\n",
    "  # shuffling and sampling to save training time\n",
    "  indices = random.sample(range(1, len(all_prefixes)), 10240)\n",
    "  prefix_sample = list(map(lambda i: all_prefixes[i], indices))\n",
    "  next_char_sample = list(map(lambda i: all_next_chars[i], indices))\n",
    "\n",
    "  return {\n",
    "      \"inputs\": prefix_sample,\n",
    "      \"labels\": next_char_sample\n",
    "  }\n",
    "\n",
    "def make_fixed_len_dataset(json_data_path: str, n=20, n_samples=10240):\n",
    "    all_lines = []\n",
    "    with open(json_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            all_lines.append(item[\"text\"])\n",
    "    all_prefixes = []\n",
    "    all_next_chars = []\n",
    "    for line in all_lines:\n",
    "        prefixes = [line[i-n:i] + \"\\n\" for i in range(n, len(line))]\n",
    "        next_chars = [line[i] + \"\\n\" for i in range(n, len(line))]\n",
    "        all_prefixes.extend(prefixes)\n",
    "        all_next_chars.extend(next_chars)\n",
    "    \n",
    "    # shuffling and sampling to save training time\n",
    "    indices = random.sample(range(1, len(all_prefixes)), n_samples)\n",
    "    prefix_sample = list(map(lambda i: all_prefixes[i], indices))\n",
    "    next_char_sample = list(map(lambda i: all_next_chars[i], indices))\n",
    "\n",
    "    return {\n",
    "        \"inputs\": prefix_sample,\n",
    "        \"labels\": next_char_sample\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"../json_data/train_data.jsonl\"\n",
    "val_data_path = \"../json_data/val_data.jsonl\"\n",
    "test_data_path = \"../json_data/test_data.jsonl\"\n",
    "\n",
    "n_samples = {\n",
    "  \"train\": 10240,\n",
    "  \"val\": 1024,\n",
    "  \"test\": 1024,\n",
    "}\n",
    "n = 20\n",
    "output_dir = f\"../data_prev_{n}\"\n",
    "for split,json_data_path in zip([\"train\", \"val\", \"test\"], [train_data_path, val_data_path, test_data_path]):\n",
    "  os.makedirs(f\"{output_dir}/{split}\", exist_ok=True)\n",
    "  data_dict = make_fixed_len_dataset(json_data_path, n=n, n_samples=n_samples[split])\n",
    "  with open(f\"{output_dir}/{split}/{split}_inputs.txt\", \"w\", encoding=\"utf-8\") as input_f:\n",
    "    input_f.writelines(data_dict[\"inputs\"])\n",
    "  with open(f\"{output_dir}/{split}/{split}_labels.txt\", \"w\", encoding=\"utf-8\") as labels_f:\n",
    "    labels_f.writelines(data_dict[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse447-hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
